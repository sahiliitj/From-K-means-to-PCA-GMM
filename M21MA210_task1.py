# -*- coding: utf-8 -*-
"""Task1_ML_A2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u0dqXh735Gu5G9CIX-_0zknUG-mP7mMn
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

tr_dp = '/content/drive/MyDrive/ML/mnist_train.csv'
ts_dp = '/content/drive/MyDrive/ML/mnist_test.csv'

d_tr = pd.read_csv(tr_dp)
d_ts = pd.read_csv(ts_dp)

print(" Here is the sample data for both the datasets (training and testing )")
print("\n TRAINING DATA SET\n")
print(d_tr.head(10))
print("\n TEST DATA SET\n")
print(d_ts.head(10))

# This code is used to check the occurences of different labels in training dataset
import seaborn as sns
plt.figure(figsize = (10,8))
sns.countplot(x = 'label', data = d_tr)
plt.xlabel("Label Digit of the Image")
plt.ylabel("Count of the label digit")
plt.show()

import seaborn as sns
plt.figure(figsize = (10,8))
sns.countplot(x = 'label', data = d_ts)
plt.xlabel("Label Digit of the Image")
plt.ylabel("Count of the label digit")
plt.show()

d_tr= d_tr.drop(columns=['label'])
d_tr.describe()

import matplotlib.pyplot as plt
import seaborn as sns
# d_tr = d_tr.drop(columns=['label'])
B = d_tr.values
pixels= B.reshape(-1)
plt.figure(figsize = (10,8))
sns.histplot(pixels, kde= False, color= "red")
plt.title('Pixels Intensity Distribution among images in the given dataset')
plt.xlabel('Intensity of pixels')
plt.ylabel('Count')
plt.show()

corr_mat = d_tr.corr()
plt.figure(figsize = (15,10))
sns.heatmap(corr_mat, cmap = 'Accent', center = 0)

# Here we are converting the dataframes into the  numpy arrays
# for the process of clustering (so as to store image in a single 1D array)
d_tr = pd.read_csv(tr_dp)
d_ts = pd.read_csv(ts_dp)
X = d_tr.values
Y = d_ts.values.astype(int)

# In this step we are initializing the centroids of the clusters
# randomly from the given dataset and then we will update them accordingly
k_clust = int(input("Enter the desired number of clusters you want:- "))
print(f"You wish to obtain {k_clust} clusters")
mu_k0 = np.random.rand(k_clust,X.shape[1]) #The first mean points

#In order to assign each data point a cluster, we need distance measure
#defining the similarity measures, in our case, its cosine measure
#which is our similarity measure.
def sim_meas(v1, v2):
    nv1,nv2 = np.linalg.norm(v1),np.linalg.norm(v2)
    dp = np.dot(v1, v2)
    dist = dp/ (nv1 * nv2)
    return dist

distance = np.zeros((X.shape[0],k_clust))
print(np.shape(distance))
mu_ki = np.zeros((k_clust,X.shape[1]))
if (np.linalg.norm(mu_k0-mu_ki).all()>0.001): # We are checking for the convergence of means of the clusters
  for t in range(150): # Here we are giving the upper bound to the number of iterations as 100
# Here we are now iterating over the number of clusters
    for i  in range(k_clust):
      for j in range(X.shape[0]): # iterating over the number of data points in our dataset
        distance[j,i] = 1- sim_meas(X[j],mu_k0[i])
# After computing the distances from data point and means, now we allot the clusters to the data points
# It is E step for EM algorithm
  allot_clusters = np.argmin(distance, axis=1)
# updating the means of the clusters
# It is M step for EM algorithm
  for i in range(k_clust):
     in_C = X[allot_clusters == i]
     if(len(in_C)) >0 :
         mu_ki = np.mean(in_C, axis = 0)
else:
  mu_k0 = mu_ki

#In this step, we are printing the data points assigned in their respective clusters using their indices
for cl_ind in range(k_clust) :
  data_point_number = np.where(allot_clusters == cl_ind)[0]
  print(f'Cluster number {cl_ind} contains the following data point having their indices :\n {data_point_number} ')

print("Now using this code, you can enter index of any data point and it will show the image corresponding to that datapoint")
point = int(input("\nEnter the index for required datapoint you want to see image:- "))
import matplotlib.pyplot as plt
plt.imshow(X[point][1:,].reshape(28,28),cmap='gray' )

import matplotlib.pyplot as plot
#Visualisation of the clusters
for kth_clust in range(k_clust):
    print(f" Cluster {kth_clust} has {np.sum(allot_clusters==kth_clust)} number of data points in it and the clusters so formed are shown below:- ")
    sample_in_clust = X[allot_clusters == kth_clust]
    sum_sample = sample_in_clust.shape[0]
    plot.figure(figsize=(10,10))
    for i in range(min(10,sum_sample)):
        plot.subplot(1,10,i+1)
        plot.imshow(sample_in_clust[i][:-1].reshape(28,28), cmap = 'gray')
        plot.axis('off')
    plot.show()

"""**Bonus Question**"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA as pca

# Here we are fetching the datasets again
d_tr = pd.read_csv(tr_dp)
d_ts = pd.read_csv(ts_dp)
d_tr= d_tr.drop(columns=['label'])
X = d_tr

# Here we will apply PCA for dimension Reduction
num_cpt = int(input("Enter the number of components you want to take for PCA:- "))
print("\n")
print(f'Performing PCA with {num_cpt}')
red_X = pca(n_components = num_cpt)
trans_X = red_X.fit_transform(X)
err=[]
for i in range(1,11):
    kmc= KMeans(n_clusters =i)
    kmc.fit(trans_X)
    err.append(kmc.inertia_)

# Plotting the elow curve to find the optimal number of clusters
plt.figure(figsize = (8,5))
plt.subplot(1,2,1)
plt.plot(range(1,11), err, linestyle= '-')
plt.xlabel( 'K- value(Number of clusters)')
plt.title('Finding Optimal Number of clusters using Elbow Method')